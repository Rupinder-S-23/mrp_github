{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Vjh6YfXffob"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('gdrive/My Drive/CompleteData.csv')\n",
        "\n",
        "columns_to_drop = ['DEP_HOUR',\n",
        "'MKT_UNIQUE_CARRIER',\n",
        "'MKT_CARRIER_FL_NUM',\n",
        "'OP_UNIQUE_CARRIER',\n",
        "'OP_CARRIER_FL_NUM',\n",
        "'TAIL_NUM',\n",
        "'ORIGIN',\n",
        "'DEST',\n",
        "'DEP_TIME',\n",
        "'CRS_DEP_TIME',\n",
        "'TAXI_OUT',\n",
        "'DEP_DELAY',\n",
        "'AIR_TIME',\n",
        "'DISTANCE',\n",
        "'LATITUDE',\n",
        "'LONGITUDE',\n",
        "'ELEVATION',\n",
        "'MESONET_STATION',\n",
        "'YEAR OF MANUFACTURE',\n",
        "'MANUFACTURER',\n",
        "'ICAO TYPE',\n",
        "'RANGE',\n",
        "'WIDTH',]\n",
        "\n",
        "df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "# drop rows with missing values\n",
        "df = df.dropna(how='any')\n",
        "\n",
        "\n",
        "df.to_csv(\"gdrive/My Drive/Cleaned_Data.csv\", index=False)\n",
        "\n",
        "data = pd.read_csv('gdrive/My Drive/Cleaned_Data.csv')"
      ],
      "metadata": {
        "id": "lQ0twaOrfnfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first few rows of the DataFrame\n",
        "data.head()\n",
        "\n",
        "# Check for missing values\n",
        "data.isnull().sum()\n",
        "\n",
        "# Explore the statistics of the dataset\n",
        "data.describe()\n",
        "\n",
        "# Explore the correlation between variables\n",
        "data.corr()"
      ],
      "metadata": {
        "id": "VGoi8lKLhFk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EDA Graphs Code\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "import seaborn as sns\n",
        "import calendar\n",
        "\n",
        "\n",
        "\n",
        "# Set the directory path to save the visuals\n",
        "save_dir = \"gdrive/My Drive/\"\n",
        "\n",
        "reason_mapping = {\n",
        "    0: 'Not Cancelled',\n",
        "    1: 'Carrier Cancellation',\n",
        "    2: 'Weather Cancellation',\n",
        "    3: 'National Air System Cancellation',\n",
        "    4: 'Security Cancellation'\n",
        "}\n",
        "\n",
        "# Map the cancellation reasons using the dictionary\n",
        "data['CANCELLED'] = data['CANCELLED'].map(reason_mapping)\n",
        "\n",
        "\n",
        "active_mapping = {\n",
        "    0: 'No weather events present',\n",
        "    1: 'Weather event(s) present',\n",
        "    2: 'Significant weather event(s)',\n",
        "\n",
        "}\n",
        "\n",
        "# Map the ACTIVE_WEATHER using the dictionary\n",
        "data['ACTIVE_WEATHER'] = data['ACTIVE_WEATHER'].map(active_mapping)\n",
        "\n",
        "\n",
        "data['FL_DATE'] = pd.to_datetime(data['FL_DATE'])\n",
        "data['Year'] = data['FL_DATE'].dt.year\n",
        "#data['Month'] = data['FL_DATE'].dt.month\n",
        "data['Month'] = data['FL_DATE'].dt.month.apply(lambda x: calendar.month_abbr[x])\n",
        "data['Day'] = data['FL_DATE'].dt.day\n",
        "\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "# Get summary statistics of numerical columns\n",
        "print(data.describe())\n",
        "# Get information about the columns, data types, and missing values\n",
        "print(data.info())\n",
        "\n",
        "# Check for missing values in each column\n",
        "print(data.isnull().sum())\n",
        "\n",
        "temperature_ranges = [\n",
        "    (-30, -20),\n",
        "    (-20, -10),\n",
        "    (-10, 0),\n",
        "    (0, 10),\n",
        "    (10, 20),\n",
        "    (20, 30),\n",
        "    (30, 40)\n",
        "]\n",
        "\n",
        "# Function to assign temperature ranges\n",
        "def get_temperature_range(temperature):\n",
        "    for min_temp, max_temp in temperature_ranges:\n",
        "        if min_temp <= temperature < max_temp:\n",
        "            return f'{min_temp}-{max_temp}'\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.countplot(x='ACTIVE_WEATHER', hue='CANCELLED', data=data)\n",
        "plt.title('Cancellation Type by Active Weather')\n",
        "plt.xlabel('Active Weather')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Cancellation Type',bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "# Format y-axis ticks to show full numbers\n",
        "plt.gca().yaxis.set_major_formatter(mticker.StrMethodFormatter('{x:,.0f}'))\n",
        "#plt.savefig(save_dir + 'cancellation_by_active_weather.png')  # Save the visualization as an image file\n",
        "plt.show()\n",
        "\n",
        "# Count occurrences of each cancellation type in each temperature range\n",
        "act_cancellation_counts = data.groupby(['ACTIVE_WEATHER', 'CANCELLED']).size().unstack(fill_value=0)\n",
        "# Create a summary table from the counts\n",
        "act_summary_table = act_cancellation_counts.reset_index()\n",
        "# Display the summary table\n",
        "print(act_summary_table)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.countplot(x='VISIBILITY', hue='CANCELLED', data=data)\n",
        "plt.title('Cancellation Type by Visibilty')\n",
        "plt.xlabel('Visibilty')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Cancellation Type',bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "# Format y-axis ticks to show full numbers\n",
        "plt.gca().yaxis.set_major_formatter(mticker.StrMethodFormatter('{x:,.0f}'))\n",
        "plt.savefig(save_dir + 'cancellation_by_Visibilty.png')  # Save the visualization as an image file\n",
        "plt.show()\n",
        "\n",
        "# Count occurrences of each cancellation type in each temperature range\n",
        "vis_cancellation_counts = data.groupby(['VISIBILITY', 'CANCELLED']).size().unstack(fill_value=0)\n",
        "# Create a summary table from the counts\n",
        "vis_summary_table = vis_cancellation_counts.reset_index()\n",
        "# Display the summary table\n",
        "print(vis_summary_table)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Apply the function to create a new column 'Temperature Range'\n",
        "data['Temp_Range'] = data['TEMPERATURE'].apply(get_temperature_range)\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.countplot(x='Temp_Range', hue='CANCELLED', data=data,\n",
        "              order=[f'{min_temp}-{max_temp}' for min_temp, max_temp in temperature_ranges])\n",
        "plt.title('Cancellation Type by Temperature')\n",
        "plt.xlabel('Temperature')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Cancellation Type',bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "# Format y-axis ticks to show full numbers\n",
        "plt.gca().yaxis.set_major_formatter(mticker.StrMethodFormatter('{x:,.0f}'))\n",
        "plt.savefig(save_dir + 'Temperature.png')  # Save the visualization as an image file\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Count occurrences of each cancellation type in each temperature range\n",
        "temperature_cancellation_counts = data.groupby(['Temp_Range', 'CANCELLED']).size().unstack(fill_value=0)\n",
        "# Create a summary table from the counts\n",
        "summary_table = temperature_cancellation_counts.reset_index()\n",
        "# Display the summary table\n",
        "print(summary_table)\n",
        "\n",
        "\n",
        "weather_features = ['WIND_SPD', 'VISIBILITY', 'TEMPERATURE', 'DEW_POINT', 'REL_HUMIDITY', 'ALTIMETER', 'CLOUD_COVER']\n",
        "correlation_matrix = data[weather_features].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap of Weather Features')\n",
        "plt.tight_layout()\n",
        "plt.savefig(save_dir + 'corr_weather_features.png')  # Save the visualization as an image file\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Example: Bar plot of cancellation reasons\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='CANCELLED', data=data)\n",
        "plt.title('Cancellation Type')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "# Format y-axis ticks to show full numbers\n",
        "plt.gca().yaxis.set_major_formatter(mticker.StrMethodFormatter('{x:,.0f}'))\n",
        "plt.savefig(save_dir + 'barplot_cancel_types.png')  # Save the visualization as an image file\n",
        "plt.show()\n",
        "\n",
        "# Filter out rows where the cancellation status is \"Not Cancelled\"\n",
        "cancelled_data = data[data['CANCELLED'] != 'Not Cancelled']\n",
        "# Group the filtered data by month and cancellation status\n",
        "cancellations_by_month = cancelled_data.groupby('Month')['CANCELLED'].value_counts().unstack()\n",
        "# Create the stacked bar plot\n",
        "plt.figure(figsize=(32, 20))\n",
        "# Sort months chronologically\n",
        "months_in_order = [calendar.month_abbr[i] for i in range(1, 13)]\n",
        "cancellations_by_month = cancellations_by_month.reindex(months_in_order)\n",
        "ax = cancellations_by_month.plot(kind='bar', stacked=True)\n",
        "plt.title('Cancellations by Month')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Count')\n",
        "# Move the legend outside the plot area to the left and at the bottom\n",
        "legend = ax.legend(title='Cancellation Type', bbox_to_anchor=(0, -0.3), loc='upper left')\n",
        "# Adjust layout to make room for the legend\n",
        "plt.subplots_adjust(bottom=0.2)  # Increase the bottom margin to make space for the legend\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig(save_dir + 'cancels_by_month.png')  # Save the visualization as an image file\n",
        "plt.show()\n",
        "\n",
        "# Count occurrences of each cancellation type in each temperature range\n",
        "month_cancellation_counts = data.groupby(['Month', 'CANCELLED']).size().unstack(fill_value=0)\n",
        "# Create a summary table from the counts\n",
        "month_table = month_cancellation_counts.reset_index()\n",
        "# Display the summary table\n",
        "print(month_table)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.countplot(x='ACTIVE_WEATHER', hue='CANCELLED', data=cancelled_data)\n",
        "plt.title('Cancellation Type by Active Weather')\n",
        "plt.xlabel('Active Weather')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Cancellation Type',bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(save_dir + 'not-cancel_cancellation_by_active_weather.png')  # Save the visualization as an image file\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.countplot(x='VISIBILITY', hue='CANCELLED', data=cancelled_data)\n",
        "plt.title('Cancellation Type by Visibilty')\n",
        "plt.xlabel('Visibilty')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Cancellation Type',bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(save_dir + 'not-cancel_cancellation_by_Visibilty.png')  # Save the visualization as an image file\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Apply the function to create a new column 'Temperature Range'\n",
        "cancelled_data['Temperature Range'] = cancelled_data['TEMPERATURE'].apply(get_temperature_range)\n",
        "\n",
        "# Create the stacked bar plot using the new temperature range column\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.countplot(x='Temperature Range', hue='CANCELLED', data=cancelled_data,\n",
        "              order=[f'{min_temp}-{max_temp}' for min_temp, max_temp in temperature_ranges])\n",
        "plt.title('Cancellation Type by Temperature Ranges')\n",
        "plt.xlabel('Temperature Range')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Cancellation Type',bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(save_dir + 'not-cancel_Temperature.png')  # Save the visualization as an image file\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "HtHdiHVhgEOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "df = data\n",
        "\n",
        "# Preprocess datetime variables\n",
        "df['FL_DATE'] = pd.to_datetime(df['FL_DATE'])\n",
        "df['year'] = df['FL_DATE'].dt.year\n",
        "df['month'] = df['FL_DATE'].dt.month\n",
        "df['day'] = df['FL_DATE'].dt.day\n",
        "\n",
        "\n",
        "features = list(df.columns)  # Add other features\n",
        "features.remove('CANCELLED')\n",
        "features.remove('FL_DATE')\n",
        "\n",
        "target = 'CANCELLED'  # the target variable\n",
        "print(features)"
      ],
      "metadata": {
        "id": "bMA31WH5gu0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RANDOM FOREST CODE\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the random forest model\n",
        "rf_model = RandomForestClassifier(verbose=2)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
        "# Make predictions on the testing set\n",
        "y_pred_prob = rf_model.predict_proba(X_test)  # Replace rf_model with your trained model\n",
        "# Calculate log loss\n",
        "logloss = log_loss(y_test, y_pred_prob)\n",
        "\n",
        "\n",
        "\n",
        "# Calculate error rates\n",
        "error_rate = 1 - accuracy\n",
        "precision_error = 1 - precision_micro\n",
        "recall_error = 1 - recall_micro\n",
        "f1_error = 1 - f1_micro\n",
        "\n",
        "# Print the evaluation metrics and error rates\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Error Rate:\", error_rate)\n",
        "print(\"Precision (Micro):\", precision_micro)\n",
        "print(\"Precision Error:\", precision_error)\n",
        "print(\"Recall (Micro):\", recall_micro)\n",
        "print(\"Recall Error:\", recall_error)\n",
        "print(\"F1-score (Micro):\", f1_micro)\n",
        "print(\"F1-score Error:\", f1_error)\n",
        "# Print log loss\n",
        "print(\"Log Loss:\", logloss)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "\n",
        "# Display the classification report\n",
        "print(report)"
      ],
      "metadata": {
        "id": "hPoeL5EOhqBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic regression\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# import data\n",
        "df = pd.read_csv('gdrive/My Drive/CompleteData.csv')\n",
        "# drop rows with missing values\n",
        "df = df.dropna(how='any')\n",
        "\n",
        "# Convert datetime column to pandas datetime format (if needed)\n",
        "df['FL_DATE'] = pd.to_datetime(df['FL_DATE'])\n",
        "\n",
        "# Set datetime column as the index\n",
        "df.set_index('FL_DATE', inplace=True)\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "\n",
        "X = df[['WIND_DIR', 'WIND_SPD', 'WIND_GUST', 'VISIBILITY', 'TEMPERATURE',\n",
        "        'DEW_POINT', 'REL_HUMIDITY', 'ALTIMETER', 'LOWEST_CLOUD_LAYER', 'N_CLOUD_LAYER',\n",
        "        'LOW_LEVEL_CLOUD', 'MID_LEVEL_CLOUD', 'HIGH_LEVEL_CLOUD',\n",
        "        'CLOUD_COVER', 'ACTIVE_WEATHER',]]  # Select relevant weather-related features\n",
        "\n",
        "\n",
        "y = df['CANCELLED']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of the Logistic Regression model\n",
        "logreg = LogisticRegression(multi_class='ovr')  # Use OvR strategy for multi-class classification\n",
        "\n",
        "# Fit the model to the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Output the classification report\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report)"
      ],
      "metadata": {
        "id": "A2-Tou3lhsaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the MLP classifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, random_state=42, verbose=True)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "# Generate classification report\n",
        "classification_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report)"
      ],
      "metadata": {
        "id": "1jnJDE_yh93S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['CANCELLED'] != 0]\n",
        "mapping = {\n",
        "    1: 'carrier_cancelled',\n",
        "    2: 'weather_cancelled',\n",
        "    3: 'national_air_system_cancelled',\n",
        "    4: 'security_cancelled'\n",
        "}\n",
        "\n",
        "# Rename values in 'CANCELLED' column based on the mapping\n",
        "data['CANCELLED'] = data['CANCELLED'].replace(mapping)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(data)\n",
        "\n",
        "data.to_csv('gdrive/My Drive/only_non_cancelled_type_data.csv', index=False)"
      ],
      "metadata": {
        "id": "UOB9x6q3iBco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first few rows of the DataFrame\n",
        "data.head()\n",
        "\n",
        "# Check for missing values\n",
        "data.isnull().sum()\n",
        "\n",
        "# Explore the statistics of the dataset\n",
        "data.describe()\n",
        "\n",
        "# Explore the correlation between variables\n",
        "data.corr()"
      ],
      "metadata": {
        "id": "bc-2KDAaiaHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  RF model 1 default hyperparameters\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the random forest model\n",
        "rf_model = RandomForestClassifier(verbose=2)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Display the classification report\n",
        "print(report)"
      ],
      "metadata": {
        "id": "-7lCauPkieCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  RF model 2 for better results attempt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the random forest model with hyperparameter tuning # 83, 56, 67\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,  # Increase the number of trees\n",
        "    max_depth=10,      # Limit tree depth to reduce overfitting\n",
        "    min_samples_split=5,  # Minimum samples required to split a node\n",
        "    min_samples_leaf=2,   # Minimum samples required to be a leaf node\n",
        "    class_weight='balanced',  # Adjust class weights for imbalanced data\n",
        "    random_state=42,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Display the classification report\n",
        "print(report)"
      ],
      "metadata": {
        "id": "jogn8_FUik1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  RF model 3 for better results attempt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the random forest model with hyperparameter tuning # 83, 65, 73\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,  # Increase the number of trees\n",
        "    max_depth=15,      # Limit tree depth to reduce overfitting\n",
        "    min_samples_split=10,  # Minimum samples required to split a node\n",
        "    min_samples_leaf=2,   # Minimum samples required to be a leaf node\n",
        "    class_weight='balanced',  # Adjust class weights for imbalanced data\n",
        "    random_state=42,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Display the classification report\n",
        "print(report)"
      ],
      "metadata": {
        "id": "sf0sO4Dsimlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  RF model 4 for better results attempt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the random forest model with hyperparameter tuning # 81, 69, 74\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,  # Increase the number of trees\n",
        "    max_depth=25,      # Limit tree depth to reduce overfitting\n",
        "    min_samples_split=10,  # Minimum samples required to split a node\n",
        "    min_samples_leaf=2,   # Minimum samples required to be a leaf node\n",
        "    class_weight='balanced',  # Adjust class weights for imbalanced data\n",
        "    random_state=42,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Display the classification report\n",
        "print(report)"
      ],
      "metadata": {
        "id": "Jpjzt-zqinE2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}